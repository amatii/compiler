{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dict={token:TOKEN,....}\n",
    "import copy,re,collections\n",
    "Token = collections.namedtuple('Token', ['type', 'value'])\n",
    "d_keyword={'main':\"main_t\",\"var\":\"var_t\",\"array\":\"array_t\",\"function\":\"function_t\",\\\n",
    "           \"procedure\":\"procedure_t\",\"return\":\"return_t\",\"while\":\"while_t\",\"do\":\"do_t\",\"od\":\"od_t\",\\\n",
    "           \"if\":\"if_t\",\"then\":\"then_t\",\"else\":\"else_t\",\"fi\":\"fi_t\",\"call\":\"call_t\",\"let\":\"let_t\"\\\n",
    "          }\n",
    "l_kewords=[i for i,v in d_keyword.items()]\n",
    "\n",
    "token_specification = [\n",
    "    ('COMMENT',   r'(#|//)(.*)(\\n)'), # Assignment operator\n",
    "    ('ASSIGN',   r'<-'),           # Assignment operator\n",
    "    ('OPEN_P',   r'\\('),           # Assignment operator\n",
    "    ('CLOSE_P',   r'\\)'),           # Assignment operator\n",
    "    ('OPEN_C',   r'{'),           # Assignment operator\n",
    "    ('CLOSE_C',   r'}'),           # Assignment operator\n",
    "    ('OPEN_B',   r'\\['),           # Assignment operator\n",
    "    ('CLOSE_B',   r'\\]'),           # Assignment operator\n",
    "    ('SEMI',   r';'),           # Assignment operator\n",
    "    ('COMMA',   r'\\,'),           # Assignment operator\n",
    "    ('POINT',   r'\\.'),           # Assignment operator\n",
    "    ('OP_EQ',      r'(==)'),            # Statement terminator\n",
    "    ('OP_NEQ',   r'(!=)'),           # Assignment operator\n",
    "    ('OP_LEQ',   r'(<=)'),           # Assignment operator\n",
    "    ('OP_GEREQ',   r'(>=)'),           # Assignment operator\n",
    "    ('OP_LESS',      r'(<)'),            # Statement terminator    \n",
    "    ('OP_GRT',      r'(>)'),            # Statement terminator    \n",
    "    ('NUMBER',   r'\\d+(?![A-Za-z])'),  #Integer\n",
    "    ('INDENT',       r'[A-Za-z]+[A-Za-z0-9]*'),    # Identifiers\n",
    "    ('OP',       r'[+\\-*/](?![+\\-*/])'),      # Arithmetic operators\n",
    "    ('NEWLINE',  r'(\\n|\\r)'),           # Line endings\n",
    "    ('SKIP',     r'[ \\t]+'),       # Skip over spaces and tabs\n",
    "    ('MISMATCH', r'.'),            # Any other character\n",
    "]\n",
    "\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self,keywords,token_specification):\n",
    "        self.keywords=keywords\n",
    "        self.token_specification=token_specification\n",
    "        self.result=[]\n",
    "        \n",
    "    def tokenize_program(self,code):\n",
    "        result=[]\n",
    "        tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in self.token_specification)\n",
    "        #print(tok_regex)\n",
    "        line_num = 1\n",
    "        line_start = 0\n",
    "        for mo in re.finditer(tok_regex, code):\n",
    "            kind = mo.lastgroup\n",
    "            value = mo.group()\n",
    "            if kind == 'NUMBER':\n",
    "                value = float(value) if '.' in value else int(value)\n",
    "            elif kind == 'INDENT' and value in self.keywords:\n",
    "                kind = value\n",
    "            elif kind == 'NEWLINE':\n",
    "                #print(\"\\n\")\n",
    "                continue\n",
    "            elif kind == 'SKIP':\n",
    "                continue\n",
    "            elif kind == 'MISMATCH':\n",
    "                #raise RuntimeError(f'{value!r} unexpected on line {line_num}')\n",
    "                print(\"========MISMATCH\",value)\n",
    "                pass\n",
    "            #yield Token(kind, value, line_num, column)\n",
    "\n",
    "            #print(Token(kind, value, line_num, column))\n",
    "            #print(kind),\n",
    "            self.result.append(Token(kind,value))\n",
    "        \n",
    "        \n",
    "    def print_tokens(self):\n",
    "        pass      \n",
    "    def detect_comment(self,line):\n",
    "        a=line.rstrip().split(\"#\")\n",
    "        return a[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "COMMENT main \n",
      "\n",
      "array OPEN_B NUMBER CLOSE_B OPEN_B NUMBER CLOSE_B OPEN_B NUMBER CLOSE_B OPEN_B NUMBER CLOSE_B INDENT SEMI \n",
      "\n",
      "var INDENT COMMA INDENT COMMA INDENT SEMI \n",
      "\n",
      "COMMENT COMMENT COMMENT COMMENT COMMENT COMMENT COMMENT COMMENT COMMENT COMMENT POINT \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f=open(\"test.c\", \"r\")\n",
    "\n",
    "tk=Tokenizer(l_kewords,token_specification)\n",
    "\n",
    "code='''\n",
    "# Array testing\n",
    "main\n",
    "array [ 5 ][ 10 ][ 15 ][ 20 ] a;\n",
    "var b, c, d;\n",
    "#function foo( );\n",
    "#{\n",
    "#\treturn 14\n",
    "#};\n",
    "#{\n",
    "#\tlet b <- 1;\n",
    "#\tlet c <- 19;\n",
    "#\tlet a[ 4 ][ 9 ][ 1 + call foo( ) ][ b * c ] <- 45;\n",
    "#\tlet d <- a[ 4 ][ 9 ][ call foo( ) + 1 ][ c * b ] + 2\n",
    "#}\n",
    ".\n",
    "'''\n",
    "a=\"\".join([lines for lines in f])\n",
    "tk.tokenize_program(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules={\n",
    "        \"computation\":\"\"\"\"main\" { varDecl } { funcDecl } \"OPEN_C\" statSequence \"CLOSE_C\" \"POINT\" \"\"\",\n",
    "        \"typeDecl\" : \"\"\" \"var\" | \"array\" \"OPEN_B\" \"NUMBER\" \"CLOSE_B\" { \"OPEN_B\" \"NUMBER\" \"CLOSE_B\" } \"\"\",\n",
    "        \"varDecl\" : \"\"\" typeDecl indent { \"COMMA\" ident } \"SEMI\" \"\"\",\n",
    "        \"funcDecl\" : \"\"\" ( \"function\" | \"procedure\" ) ident [ formalParam ] \"SEMI\" funcBody \"SEMI\" \"\"\",\n",
    "        \"formalParam\" : \"\"\" \"OPEN_P\" [ ident { \"COMMA\" ident } ] \"CLOSE_P\" \"\"\",\n",
    "        \"funcBody\" : \"\"\" { varDecl } \"OPEN_B\" [ statSequence ] \"CLOSE_B\" \"\"\"              \n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "main \n",
      "\n",
      "var INDENT COMMA INDENT COMMA INDENT SEMI \n",
      "\n",
      "OPEN_C \n",
      "\n",
      "if INDENT OP_LESS INDENT then \n",
      "\n",
      "let INDENT ASSIGN INDENT OP INDENT \n",
      "\n",
      "else \n",
      "\n",
      "let INDENT ASSIGN INDENT OP INDENT \n",
      "\n",
      "fi \n",
      "\n",
      "CLOSE_C \n",
      "\n",
      "POINT \n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import inspect\n",
    "\n",
    "pair={\"[\":\"]\",\"{\":\"}\",\"(\":\")\"}\n",
    "class parser():\n",
    "    def __init__(self,tokens,rules):\n",
    "        self.tokens=collections.deque([i for i in tokens if i.type!='COMMENT'])\n",
    "        self.rules=rules\n",
    "        #for i in self.tokens:\n",
    "        #    print(i.type)\n",
    "    \n",
    "    #computation\n",
    "    def check_computation(self,b):\n",
    "        self.must_parse(\"main\")\n",
    "        #more than 0\n",
    "        while self.tokens[0].type in [\"var\",\"array\"]:\n",
    "            self.check_varDecl()\n",
    "        #more than 0\n",
    "        while self.tokens[0].type in [\"function\",\"procedure\"]:\n",
    "            self.check_funcDecl()\n",
    "        self.must_parse(\"OPEN_C\")        \n",
    "        self.check_stat_Sequence()\n",
    "        self.must_parse(\"CLOSE_C\")\n",
    "        self.must_parse(\"POINT\")\n",
    "        \n",
    "    def check_funcDecl(self):\n",
    "        if self.tokens[0].type==\"function\":\n",
    "            self.must_parse(\"function\")\n",
    "        else:\n",
    "            self.must_parse(\"procedure\")\n",
    "        self.must_parse(\"INDENT\")\n",
    "        self.check_formalParam()\n",
    "        self.must_parse(\"SEMI\")\n",
    "        self.check_funcBody()\n",
    "        self.must_parse(\"SEMI\")\n",
    "        \n",
    "    def check_formalParam(self):\n",
    "        if self.tokens[0].type==\"OPEN_P\":\n",
    "            self.must_parse(\"OPEN_P\")\n",
    "            if self.tokens[0].type==\"INDENT\":\n",
    "                self.must_parse(\"INDENT\")\n",
    "                #more than 0\n",
    "                while self.tokens[0].type==\"COMMA\":            \n",
    "                    self.must_parse(\"COMMA\")\n",
    "                    self.must_parse(\"INDENT\")\n",
    "            self.must_parse(\"CLOSE_P\")\n",
    "         \n",
    "    def check_funcBody(self):\n",
    "        #more than 0\n",
    "        while self.tokens[0].type in [\"var\",\"array\"]:\n",
    "            self.check_varDecl()\n",
    "        self.must_parse(\"OPEN_C\")        \n",
    "        self.check_stat_Sequence()\n",
    "        self.must_parse(\"CLOSE_C\")\n",
    "\n",
    "    def check_stat_Sequence(self):\n",
    "        self.check_statement()\n",
    "        # more that 0\n",
    "        while self.tokens[0].type in [\"SEMI\"]:\n",
    "            self.must_parse(\"SEMI\")            \n",
    "            self.check_statement()\n",
    "            \n",
    "    def check_statement(self):\n",
    "        if self.tokens[0].type not in [\"let\",\"call\",\"if\",\"while\",\"return\"]:\n",
    "            print(\"error check_statement\",self.tokens[0].type,inspect.stack()[1][3])\n",
    "        if self.tokens[0].type==\"let\":\n",
    "            self.check_assignment()\n",
    "        if self.tokens[0].type==\"call\":\n",
    "            self.check_funcCall()\n",
    "        if self.tokens[0].type==\"if\":\n",
    "            self.check_ifStatement()\n",
    "        if self.tokens[0].type==\"while\":\n",
    "            self.check_whileStatement()\n",
    "        if self.tokens[0].type==\"return\":\n",
    "            self.check_returnStatement()\n",
    "    def check_returnStatement(self):\n",
    "        self.must_parse(\"return\")\n",
    "        #how?\n",
    "        if self.tokens[0].type in [\"INDENT\",\"NUMBER\",\"OPEN_P\",\"call\"]:\n",
    "            self.check_expression()\n",
    "\n",
    "    def check_whileStatement(self):\n",
    "        self.must_parse(\"while\")\n",
    "        self.check_relation()\n",
    "        self.must_parse(\"do\")\n",
    "        self.check_stat_Sequence()\n",
    "        self.must_parse(\"od\")\n",
    "        \n",
    "    \n",
    "    def check_ifStatement(self):\n",
    "        self.must_parse(\"if\")\n",
    "        self.check_relation()\n",
    "        self.must_parse(\"then\")\n",
    "        self.check_stat_Sequence()\n",
    "        \n",
    "        if self.tokens[0].type==\"else\":\n",
    "            self.must_parse(\"else\")\n",
    "            self.check_stat_Sequence()\n",
    "            \n",
    "        self.must_parse(\"fi\")\n",
    "        \n",
    "    \n",
    "    def check_relation(self):\n",
    "        self.check_expression()\n",
    "        self.check_relOp()\n",
    "        self.check_expression()\n",
    "        \n",
    "    def check_relOp(self):\n",
    "        if self.tokens[0].type in ['OP_EQ','OP_NEQ','OP_LESS','OP_LEQ','OP_GRT','OP_GEREQ']:\n",
    "            self.must_parse(self.tokens[0].type)\n",
    "    \n",
    "    def check_assignment(self):\n",
    "        self.must_parse(\"let\")\n",
    "        self.check_designator()\n",
    "        self.must_parse(\"ASSIGN\")\n",
    "        self.check_expression()\n",
    "    \n",
    "    def check_designator(self):\n",
    "        self.must_parse(\"INDENT\")\n",
    "        while self.tokens[0].type==\"OPEN_B\":\n",
    "            self.must_parse(\"OPEN_B\")\n",
    "            self.check_expression()\n",
    "            self.must_parse(\"CLOSE_B\")\n",
    "    \n",
    "    \n",
    "                \n",
    "    def check_term(self):\n",
    "        self.check_factor()\n",
    "        while self.tokens[0].value in [\"*\",\"/\"]:\n",
    "            if self.tokens[0].value==\"*\":\n",
    "                self.must_parse(\"OP\")\n",
    "            if self.tokens[0].value==\"/\":\n",
    "                self.must_parse(\"OP\")\n",
    "            self.check_factor()\n",
    "    \n",
    "    def check_factor(self):\n",
    "        if self.tokens[0].type not in [\"INDENT\",\"NUMBER\",\"OPEN_P\",\"call\"]:\n",
    "            print(\"error,check_factor!!!\",self.tokens[0],inspect.stack()[1][3])\n",
    "        if self.tokens[0].type==\"INDENT\":\n",
    "            self.check_designator()\n",
    "        if self.tokens[0].type==\"NUMBER\":\n",
    "            self.must_parse(\"NUMBER\")\n",
    "        if self.tokens[0].type==\"OPEN_P\":\n",
    "            self.must_parse(\"OPEN_P\")\n",
    "            self.check_expression()\n",
    "            self.must_parse(\"CLOSE_P\")\n",
    "        if self.tokens[0].type==\"call\":\n",
    "            self.check_funcCall()\n",
    "        \n",
    "    def check_funcCall(self):\n",
    "        self.must_parse(\"call\")\n",
    "        self.must_parse(\"INDENT\")\n",
    "        if self.tokens[0].type==\"OPEN_P\":\n",
    "            self.must_parse(\"OPEN_P\")\n",
    "            #how?\n",
    "            if self.tokens[0].type in [\"INDENT\",\"NUMBER\",\"OPEN_P\",\"call\"]:\n",
    "                self.check_expression()\n",
    "                while self.tokens[0].type==\"COMMA\":\n",
    "                        self.must_parse(\"COMMA\")\n",
    "                        self.check_expression()\n",
    "            self.must_parse(\"CLOSE_P\")\n",
    "        \n",
    "    def check_varDecl(self):\n",
    "        self.check_typeDecl()\n",
    "        #print(self.tokens[0].value)\n",
    "        self.must_parse(\"INDENT\")\n",
    "        #more than 0\n",
    "        while self.tokens[0].type in [\"COMMA\"]:\n",
    "            self.must_parse(\"COMMA\")            \n",
    "            #print(self.tokens[0].value)\n",
    "            self.must_parse(\"INDENT\")\n",
    "        self.must_parse(\"SEMI\")\n",
    "        \n",
    "    def check_typeDecl(self):\n",
    "        if self.tokens[0].type==\"array\":\n",
    "            self.must_parse(\"array\")\n",
    "            counter=0\n",
    "            #more than 1\n",
    "            while self.tokens[0].type==\"OPEN_B\":\n",
    "                self.must_parse(\"OPEN_B\")\n",
    "                self.must_parse(\"NUMBER\")\n",
    "                self.must_parse(\"CLOSE_B\")\n",
    "                counter+=1\n",
    "            if counter==0:\n",
    "                print(\"error typeDecl\",self.tokens[0].type)\n",
    "                \n",
    "        elif self.tokens[0].type==\"var\":\n",
    "            self.must_parse(\"var\")\n",
    "        else:\n",
    "            print(\"error typeDecl\",target_token,self.tokens[0].type)\n",
    "    \n",
    "    def must_parse(self,target_token):        \n",
    "        sym=self.tokens.popleft()\n",
    "        if (sym.type==target_token):\n",
    "            #print(sym.value)\n",
    "            pass\n",
    "        else:\n",
    "            print(\"error!!!!!\",target_token,sym,inspect.stack()[1][3])\n",
    "            print([i.value for i in self.tokens])\n",
    "    \n",
    "    def check_expression(self):\n",
    "        self.check_term()\n",
    "        while self.tokens[0].value in [\"+\",\"-\"]:\n",
    "            if self.tokens[0].value==\"+\":\n",
    "                self.must_parse(\"OP\")\n",
    "            if self.tokens[0].value==\"-\":\n",
    "                self.must_parse(\"OP\")\n",
    "            self.check_term()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tk=Tokenizer(l_kewords,token_specification)\n",
    "\n",
    "code='''\n",
    "# Nested if/while v3\n",
    "# If I think I pass this, I'm moving on.\n",
    "# This test should not be attempted by anyone who is pregnant, nursing, has \n",
    "# high blood pressure, aliens, and stressed out graduate students.\n",
    "# Based on test 13.\n",
    "main\n",
    "var x, y;\n",
    "var a, b;\n",
    "{\n",
    "\tcall foo( );\n",
    "\tlet y <- y + call boo( )\n",
    "}\n",
    ".\n",
    "'''\n",
    "tk.tokenize_program(code)\n",
    "\n",
    "c=parser(tk.result,rules)\n",
    "c.check_computation(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test022.txt', '==========')\n",
      "('test024.txt', '==========')\n",
      "('test002.txt', '==========')\n",
      "('test015.txt', '==========')\n",
      "('test010.txt', '==========')\n",
      "('test019.txt', '==========')\n",
      "('test012.txt', '==========')\n",
      "('test029.txt', '==========')\n",
      "('test030.txt', '==========')\n",
      "('test026.txt', '==========')\n",
      "('test027.txt', '==========')\n",
      "('test021.txt', '==========')\n",
      "('test003.txt', '==========')\n",
      "('test009.txt', '==========')\n",
      "('test016.txt', '==========')\n",
      "('test017.txt', '==========')\n",
      "('test020.txt', '==========')\n",
      "('test014.txt', '==========')\n",
      "('test011.txt', '==========')\n",
      "('test004.txt', '==========')\n",
      "('factorial.txt', '==========')\n",
      "('test013.txt', '==========')\n",
      "('test018.txt', '==========')\n",
      "('test005.txt', '==========')\n",
      "('test023.txt', '==========')\n",
      "('test031.txt', '==========')\n",
      "('cell.txt', '==========')\n",
      "('test007.txt', '==========')\n",
      "('test028.txt', '==========')\n",
      "('test008.txt', '==========')\n",
      "('test025.txt', '==========')\n",
      "('test001.txt', '==========')\n",
      "('big.txt', '==========')\n",
      "('test006.txt', '==========')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path=\"/home/ahmad/Documents/compiler/project/testprogs/\"\n",
    "for files in os.listdir(path):\n",
    "    print(files,\"==========\")\n",
    "    a=\"\".join([lines for lines in open(path+files,'r')])\n",
    "    tk=Tokenizer(l_kewords,token_specification)\n",
    "    tk.tokenize_program(a)\n",
    "    c=parser(tk.result,rules)\n",
    "    c.check_computation(\"computation\")\n",
    "    #tk.tokenize_program(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
