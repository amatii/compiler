{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dict={token:TOKEN,....}\n",
    "import copy,re,collections\n",
    "Token = collections.namedtuple('Token', ['type', 'value','real_value'])\n",
    "d_keyword={'main':\"main_t\",\"var\":\"var_t\",\"array\":\"array_t\",\"function\":\"function_t\",\\\n",
    "           \"procedure\":\"procedure_t\",\"return\":\"return_t\",\"while\":\"while_t\",\"do\":\"do_t\",\"od\":\"od_t\",\\\n",
    "           \"if\":\"if_t\",\"then\":\"then_t\",\"else\":\"else_t\",\"fi\":\"fi_t\",\"call\":\"call_t\",\"let\":\"let_t\"\\\n",
    "          }\n",
    "l_kewords=[i for i,v in d_keyword.items()]\n",
    "\n",
    "token_specification = [\n",
    "    ('COMMENT',   r'(#|//)(.*)(\\n)'), # Assignment operator\n",
    "    ('ASSIGN',   r'<-'),           # Assignment operator\n",
    "    ('OPEN_P',   r'\\('),           # Assignment operator\n",
    "    ('CLOSE_P',   r'\\)'),           # Assignment operator\n",
    "    ('OPEN_C',   r'{'),           # Assignment operator\n",
    "    ('CLOSE_C',   r'}'),           # Assignment operator\n",
    "    ('OPEN_B',   r'\\['),           # Assignment operator\n",
    "    ('CLOSE_B',   r'\\]'),           # Assignment operator\n",
    "    ('SEMI',   r';'),           # Assignment operator\n",
    "    ('COMMA',   r'\\,'),           # Assignment operator\n",
    "    ('POINT',   r'\\.'),           # Assignment operator\n",
    "    ('OP_EQ',      r'(==)'),            # Statement terminator\n",
    "    ('OP_NEQ',   r'(!=)'),           # Assignment operator\n",
    "    ('OP_LEQ',   r'(<=)'),           # Assignment operator\n",
    "    ('OP_GEREQ',   r'(>=)'),           # Assignment operator\n",
    "    ('OP_LESS',      r'(<)'),            # Statement terminator    \n",
    "    ('OP_GRT',      r'(>)'),            # Statement terminator    \n",
    "    ('NUMBER',   r'\\d+(?![A-Za-z])'),  #Integer\n",
    "    ('INDENT',       r'[A-Za-z]+[A-Za-z0-9]*'),    # Identifiers\n",
    "    ('OP',       r'[+\\-*/](?![+\\-*/])'),      # Arithmetic operators\n",
    "    ('NEWLINE',  r'(\\n|\\r)'),           # Line endings\n",
    "    ('SKIP',     r'[ \\t]+'),       # Skip over spaces and tabs\n",
    "    ('MISMATCH', r'.'),            # Any other character\n",
    "]\n",
    "\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self,keywords,token_specification):\n",
    "        self.keywords=keywords\n",
    "        self.token_specification=token_specification\n",
    "        self.result=[]\n",
    "        self.indent_index_counter=0\n",
    "        self.indent_index_table=dict()\n",
    "        \n",
    "    def tokenize_program(self,code):\n",
    "        result=[]\n",
    "        tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in self.token_specification)\n",
    "        #print(tok_regex)\n",
    "        line_num = 1\n",
    "        line_start = 0\n",
    "        for mo in re.finditer(tok_regex, code):\n",
    "            kind = mo.lastgroup\n",
    "            value = mo.group()\n",
    "            real_value=None\n",
    "            if kind == 'NUMBER':\n",
    "                value = float(value) if '.' in value else int(value)\n",
    "            elif kind == 'INDENT' and value in self.keywords:\n",
    "                kind = value\n",
    "            elif kind == 'INDENT' and value not in self.keywords:\n",
    "                jit=False\n",
    "                real_value=value\n",
    "                if True:\n",
    "                    if value in self.indent_index_table.keys():\n",
    "                        value=self.indent_index_table[value]\n",
    "                    else:\n",
    "                        self.indent_index_table[value]=self.indent_index_counter\n",
    "                        self.indent_index_counter+=1\n",
    "                        value=self.indent_index_table[value]\n",
    "\n",
    "            elif kind == 'NEWLINE':\n",
    "                #print(\"\\n\")\n",
    "                continue\n",
    "            elif kind == 'SKIP':\n",
    "                continue\n",
    "            elif kind == 'MISMATCH':\n",
    "                #raise RuntimeError(f'{value!r} unexpected on line {line_num}')\n",
    "                print(\"========MISMATCH\",value)\n",
    "                pass\n",
    "            #yield Token(kind, value, line_num, column)\n",
    "\n",
    "            #print(Token(kind, value, line_num, column))\n",
    "            #print(kind),\n",
    "            self.result.append(Token(kind,value,real_value))\n",
    "        \n",
    "        \n",
    "    def print_tokens(self):\n",
    "        for t in self.result:\n",
    "            print(t.value),\n",
    "        \n",
    "    def detect_comment(self,line):\n",
    "        a=line.rstrip().split(\"#\")\n",
    "        return a[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "# Array testing\n",
      "main var 0 , 1 , 2 ; function 3 ( ) ; { return 14 } ; { let 0 <- 1 + 2 } .\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print(\"test\")\n",
    "    f=open(\"test.c\", \"r\")\n",
    "\n",
    "    tk=Tokenizer(l_kewords,token_specification)\n",
    "\n",
    "    code='''\n",
    "    # Array testing\n",
    "    main\n",
    "    var a, b, c;\n",
    "    function foo( );\n",
    "    {\n",
    "    \treturn 14\n",
    "    };\n",
    "    {\n",
    "    \tlet a <- b + c\n",
    "    }\n",
    "    .\n",
    "    '''\n",
    "    a=\"\".join([lines for lines in f])\n",
    "    tk.tokenize_program(code)\n",
    "    tk.print_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some parser rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules={\n",
    "        \"computation\":\"\"\"\"main\" { varDecl } { funcDecl } \"OPEN_C\" statSequence \"CLOSE_C\" \"POINT\" \"\"\",\n",
    "        \"typeDecl\" : \"\"\" \"var\" | \"array\" \"OPEN_B\" \"NUMBER\" \"CLOSE_B\" { \"OPEN_B\" \"NUMBER\" \"CLOSE_B\" } \"\"\",\n",
    "        \"varDecl\" : \"\"\" typeDecl indent { \"COMMA\" ident } \"SEMI\" \"\"\",\n",
    "        \"funcDecl\" : \"\"\" ( \"function\" | \"procedure\" ) ident [ formalParam ] \"SEMI\" funcBody \"SEMI\" \"\"\",\n",
    "        \"formalParam\" : \"\"\" \"OPEN_P\" [ ident { \"COMMA\" ident } ] \"CLOSE_P\" \"\"\",\n",
    "        \"funcBody\" : \"\"\" { varDecl } \"OPEN_B\" [ statSequence ] \"CLOSE_B\" \"\"\"              \n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(l_kewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
