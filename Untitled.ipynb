{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dict={token:TOKEN,....}\n",
    "import copy,re\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self,in_tok=None):\n",
    "        self.rule = copy.deepcopy(in_tok)\n",
    "        self.Del=dict()\n",
    "        \n",
    "    def add_rule(self,d):\n",
    "        for in_str,token_name in d.items():\n",
    "            if in_str not in self.Del:\n",
    "                self.Del[in_str]=token_name\n",
    "            else:\n",
    "                print(in_str,token_name, \"exists!\")\n",
    "                \n",
    "    def tokenize_program(self,program):\n",
    "        for line in program:\n",
    "            #TODO: each statement is in ONE line\n",
    "            a=self.detect_comment(line).split()\n",
    "            a=[(i,None) for i in a if i is not \" \"]\n",
    "            \n",
    "            while True:\n",
    "                for i in a:\n",
    "                    for k,v in self.Del.items():\n",
    "                        re.split(i,k)\n",
    "                        \n",
    "            \n",
    "            \n",
    "            return None\n",
    "            for sym in a:\n",
    "                if sym in self.rule:\n",
    "                    print(self.rule[sym])\n",
    "                    continue\n",
    "                for ch in sym:\n",
    "                    if ch in self.rule:\n",
    "                        print(self.rule[sym])\n",
    "                        continue\n",
    "                    if ch.isdigit():\n",
    "                        pass\n",
    "                    else:\n",
    "                        pass\n",
    "                        \n",
    "                        \n",
    "            print(a)\n",
    "            \n",
    "                \n",
    "    def detect_comment(self,line):\n",
    "        a=line.rstrip().split(\"#\")\n",
    "        return a[0]    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"test.c\", \"r\")\n",
    "d_keyword={'main':\"main_t\",\"var\":\"var_t\",\"array\":\"array_t\",\"function\":\"function_t\",\n",
    "           \"procedure\":\"procedure_t\",\"return\":\"return_t\",\"while\":\"while_t\",\"do\":\"do_t\",\"od\":\"od_t\",\n",
    "           \"if\":\"if_t\",\"then\":\"then_t\",\"else\":\"else_t\",\"fi\":\"fi_t\",\"call\":\"call_t\",\"let\":\"let_t\"\n",
    "          }\n",
    "d_PairDel={\"(\":\"open_p_t\",\")\":\"close_p_t\",\"{\":\"open_c_t\",\"}\":\"close_c_t\",\"[\":\"open_b_t\",\"]\":\"close_b_t\"}\n",
    "d_SingleDel={\";\":\"sem_t\",\",\":\"coma_t\",\".\":\"point_t\"}\n",
    "l= {\"==\":\"equal_t\" , \"!=\":\"nequal_t\" , \"<\":\"less_t\" , \"<=\":\"lequal_t\" , \">\":\"gret_t\" , \">=\":\"gequal_t\"}\n",
    "#print(d_keyword)\n",
    "tk=Tokenizer(d_keyword)\n",
    "tk.add_rule(d_PairDel)\n",
    "tk.add_rule(d_SingleDel)\n",
    "\n",
    "tk.tokenize_program(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-34ff29bb83ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# input_dict={token:TOKEN,....}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mToken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0md_keyword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"main_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"var\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"var_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"array_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"function\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"function_t\"\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;34m\"procedure\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"procedure_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"return\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"return_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"while\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"while_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"do\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"do_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"od\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"od_t\"\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;34m\"if\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"if_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"then\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"then_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"else\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"else_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fi\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"fi_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"call\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"call_t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"let\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"let_t\"\u001b[0m          \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ml_kewords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md_keyword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "# input_dict={token:TOKEN,....}\n",
    "import copy,re\n",
    "Token = collections.namedtuple('Token', ['type', 'value'])\n",
    "d_keyword={'main':\"main_t\",\"var\":\"var_t\",\"array\":\"array_t\",\"function\":\"function_t\",\\\n",
    "           \"procedure\":\"procedure_t\",\"return\":\"return_t\",\"while\":\"while_t\",\"do\":\"do_t\",\"od\":\"od_t\",\\\n",
    "           \"if\":\"if_t\",\"then\":\"then_t\",\"else\":\"else_t\",\"fi\":\"fi_t\",\"call\":\"call_t\",\"let\":\"let_t\"\\\n",
    "          }\n",
    "l_kewords=[i for i,v in d_keyword.items()]\n",
    "\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self,keywords,token_specification):\n",
    "        self.keywords=keywords\n",
    "        self.token_specification=token_specification\n",
    "        \n",
    "    def tokenize_program(self,code):\n",
    "        result=[]\n",
    "        tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in self.token_specification)\n",
    "        #print(tok_regex)\n",
    "        line_num = 1\n",
    "        line_start = 0\n",
    "        for mo in re.finditer(tok_regex, code):\n",
    "            kind = mo.lastgroup\n",
    "            value = mo.group()\n",
    "            if kind == 'NUMBER':\n",
    "                value = float(value) if '.' in value else int(value)\n",
    "            elif kind == 'ID' and value in self.keywords:\n",
    "                kind = value\n",
    "            elif kind == 'NEWLINE':\n",
    "                print(\"\\n\")\n",
    "                continue\n",
    "            elif kind == 'SKIP':\n",
    "                continue\n",
    "            elif kind == 'MISMATCH':\n",
    "                #raise RuntimeError(f'{value!r} unexpected on line {line_num}')\n",
    "                print(\"========MISMATCH\")\n",
    "                pass\n",
    "            #yield Token(kind, value, line_num, column)\n",
    "\n",
    "            #print(Token(kind, value, line_num, column))\n",
    "            print(kind),\n",
    "    \n",
    "    def print_tokens(self):\n",
    "        pass      \n",
    "    def detect_comment(self,line):\n",
    "        a=line.rstrip().split(\"#\")\n",
    "        return a[0]    \n",
    "\n",
    "token_specification = [\n",
    "    ('COMMENT',   r'\\#(.*)(\\n)'), # Assignment operator\n",
    "    ('ASSIGN',   r'<-'),           # Assignment operator\n",
    "    ('OPEN_P',   r'\\('),           # Assignment operator\n",
    "    ('CLOSE_P',   r'\\)'),           # Assignment operator\n",
    "    ('OPEN_C',   r'{'),           # Assignment operator\n",
    "    ('CLOSE_C',   r'}'),           # Assignment operator\n",
    "    ('OPEN_B',   r'\\['),           # Assignment operator\n",
    "    ('CLOSE_B',   r'\\]'),           # Assignment operator\n",
    "    ('SEMI',   r';'),           # Assignment operator\n",
    "    ('COMMA',   r'\\,'),           # Assignment operator\n",
    "    ('POINT',   r'\\.'),           # Assignment operator\n",
    "    ('OP_EQ',      r'=='),            # Statement terminator\n",
    "    ('OP_NEQ',   r'!='),           # Assignment operator\n",
    "    ('OP_LESS',      r'=='),            # Statement terminator\n",
    "    ('OP_LEQ',   r'<='),           # Assignment operator\n",
    "    ('OP_GRT',      r'\\>'),            # Statement terminator\n",
    "    ('OP_GEREQ',   r'!='),           # Assignment operator\n",
    "    ('NUMBER',   r'\\d+(?![A-Za-z])'),  #Integer\n",
    "    ('ID',       r'[A-Za-z]+[A-Za-z0-9]*'),    # Identifiers\n",
    "    ('OP',       r'[+\\-*/]'),      # Arithmetic operators\n",
    "    ('NEWLINE',  r'\\n'),           # Line endings\n",
    "    ('SKIP',     r'[ \\t]+'),       # Skip over spaces and tabs\n",
    "    ('MISMATCH', r'.'),            # Any other character\n",
    "]\n",
    "\n",
    "f=open(\"test.c\", \"r\")\n",
    "\n",
    "tk=Tokenizer(l_kewords,token_specification)\n",
    "\n",
    "code='''\n",
    "# Array testing\n",
    "main\n",
    "array [ 5 ][ 10 ][ 15 ][ 20 ] a;\n",
    "var b, c, d;\n",
    "function foo( );\n",
    "{\n",
    "\treturn 14\n",
    "};\n",
    "{\n",
    "\tlet b <- 1;\n",
    "\tlet c <- 19;\n",
    "\tlet a[ 4 ][ 9 ][ 1 + call foo( ) ][ b * c ] <- 45;\n",
    "\tlet d <- a[ 4 ][ 9 ][ call foo( ) + 1 ][ c * b ] + 2\n",
    "}\n",
    ".\n",
    "'''\n",
    "a=\"\".join([lines for lines in f])\n",
    "tk.tokenize_program(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
